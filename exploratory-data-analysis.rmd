---
title: STATS 765 - Where are the Crimes? - Milestone 2
author: Sujay Anjankar - 2433337734
toc-title: Table of Contents
---


```{r setup, include=FALSE}
## Setup
library("tidyverse")
path <- "~/Documents/Workspace/2025-s2-stats-765/data"
```

## Load Files

Special Values:

| Value  | Meaning                           |
| ------ | --------------------------------- |
| `-997` | Data not collected                |
| `-999` | Suppressed due to confidentiality |


```{r load-files, cache = TRUE}
df_victimizations <- read_csv(
    paste(path, "/victimizations-data.csv", sep = ""),
    na = c("", "NA"), show_col_types = FALSE
)

df_census <- read_csv(
    paste(path, "/2023_Census_totals_by_topic_for_households_by_SA2.csv", sep = ""),
    # Read the special values as NA for simplicity.
    na = c("", "NA", -999, -997), show_col_types = FALSE
)

df_meshblock <- read_csv(
    paste(path, "/meshblock-higher-geographies-2023-generalized.csv", sep = ""),
    na = c("", "NA"), show_col_types = FALSE
)
dim(df_census)
dim(df_meshblock)
dim(df_victimizations)
```

## Cleanup

### Victimizations

```{r cleanup-victimizations, cache = TRUE}
# Find all columns which only have 1 distinct value.
single_value_column_names <- df_victimizations |>
    summarize(across(everything(), ~ n_distinct(.))) |>
    # This anonymous function keeps only those columns in the tibble which have value = 1.
    select(where(~ 1 == .x)) |>
    names()

# Remove these columns.
df_victimizations <- df_victimizations |>
    # Rename meshblock while at it.
    rename(meshblock = Meshblock) |>
    select(-all_of(single_value_column_names))

# Year Month and Year Month (copy 2) likely to contain the same values.
print(
    df_victimizations |>
        select(c(`Year Month`, `Year Month (copy 2)`)) |>
        mutate(diff = `Year Month` != `Year Month (copy 2)`) |>
        filter(diff == TRUE) |>
        summarize(n = n())
)

# Since the above returns a 0 count, drop that column.
df_victimizations <- df_victimizations |> select(-c(`Year Month (copy 2)`)) |>
    # Extract the year to join with the census data.
    mutate(year = as.numeric(str_sub(`Year Month`, start = -4L, end = -1L))) |>
    # Remove excess columns not relevant to analysis.
    select(
        -c(`Month Year`, `Occurrence Day Of Week`, `Occurrence Hour Of Day`, `Territorial Authority`, `Location Type`)
    )

# Final list of columns
colnames(df_victimizations)
```

### Meshblock

```{r cleanup-meshblock, cache = TRUE}
colnames(df_meshblock)
df_meshblock <- df_meshblock |>
    # This dataset is to just join the two datasets, so drop any excess columns.
    select(c(MB2023_V1_00, SA22023_V1_00, SA22023_V1_00_NAME)) |>
    # The meshblock is a string here, so it needs to be converted to a numeric column before joining.
    mutate(meshblock = as.numeric(MB2023_V1_00)) |> # Rename it to meshblock for ease of joining.
    rename(
        sa2_code = SA22023_V1_00,
        sa2_name = SA22023_V1_00_NAME
    ) |>
    select(-MB2023_V1_00)

# Unlikely to have columns that only have 1 distinct values in this data, check anyway.
print(
    df_meshblock |>
        summarize(across(everything(), ~ n_distinct(.))) |>
        # This anonymous function keeps only those columns in the tibble which have value = 1.
        select(where(~ 1 == .x)) |>
        names()
) # Returns nothing.
```

### Census

```{r cleanup-census, cache = TRUE}
# The column names are excessively long, shorten them for readability.

single_value_colnames_census <- df_census |>
    summarize(across(everything(), ~ n_distinct(.))) |>
    # This anonymous function keeps only those columns in the tibble which have value = 1.
    select(where(~ 1 == .x)) |>
    names()

# Determine the distinct values available for these columns.
print(
    df_census |>
        select(all_of(single_value_colnames_census)) |>
        distinct() |>
        as.data.frame()
)

# Since all of these are NA values, meaning the data is not available, we remove these columns.
df_census <- df_census |>
    select(-all_of(single_value_colnames_census))

df_census <- df_census |>
    # Rename overly long columns.
    rename_with(
        ~ .x |>
            str_replace("Subject pop: Households in rented occupied private dwellings, Year: ", "Rentals - ") |>
            str_replace("Subject pop: Households in occupied private dwellings, Year: ", "Households - ") |>
            str_replace(", Measure: Count, Var1:", " -") |>
            str_replace(", Measure: Median, Var1: ", " - Median: ") |>
            str_replace(", Measure: Mean, Var1: ", " - Median: ") |>
            str_replace(" paid by household", "") |>
            str_replace("Total household income", "Income") |>
            str_replace("Sector of landlord", "Landlord") |>
            str_replace("Tenure of household", "Tenure") |>
            str_replace("Number of usual residents in household", "# of Residents") |>
            str_replace("Household crowding index", "Crowding Index") |>
            str_replace("Household composition", "Composition") |>
            str_replace("Access to telecommunication systems", "Telecom Access") |>
            str_replace("Number of motor vehicles", "Vehicle Count") |>
            str_replace("Count of households in occupied private dwellings", "Household Count")
    ) |>
    # Drop other columns not required to the analysis.
    select(-c(
        OBJECTID,
        `Statistical area 2 (SA2) 2023 name no macrons`,
        `Area square kilometres`,
        `Land area square kilometres`,
        `Shape__Area`,
        `Shape__Length`
    )) |>
    rename(
        sa2_code = `Statistical area 2 (SA2) 2023 code`,
        sa2_name = `Statistical area 2 (SA2) 2023 name`
    )
```

### Check Available Years

```{r filtering-by-year}
df_victimizations |>
    select(year) |>
    distinct()
# The only available victimization data is from 2023, so drop the columns from the census data from other years.
df_victimizations <- df_victimizations |> filter(year == 2023)
df_census <- df_census |>
    select(!matches("2013|2018")) |>
    # Since it is all 2023 data, remove the year from column names.
    rename_with(~ .x |> str_replace(" - 2023", ""))
```

## Join Datasets

```{r join-all, cache = TRUE}
df_merge <- inner_join(
    df_census,
    inner_join(
        df_victimizations, df_meshblock,
        by = "meshblock"
    ),
    by = c("sa2_code", "sa2_name")
)
dim(df_merge)
```

## Visualizations

### Distribution of Committed Offenses
```{r visualization-1, cache = TRUE}
# Most committed offenses
df_merge |>
    group_by(`ANZSOC Division`) |>
    summarize(count_by_subdivision = n(), .groups = "drop") |>
    ggplot(aes(x = "", y = count_by_subdivision, fill = `ANZSOC Division`)) +
    geom_bar(stat = "identity", width = 1) +
    coord_polar("y", start = 0) +
    labs(
        title = "Distribution of Committed Offenses",
        x = "",
        y = ""
    )
```

### Criminal Activity Hotspots
```{r visualization-2, cache = TRUE}
# Areas with a high prevalence of crime
knitr::kable(
    df_merge |>
        group_by(sa2_name) |>
        summarize(count_by_sa2 = n(), .groups = "drop") |>
        slice_max(n = 10, order_by = count_by_sa2),
    col.names = c("Statistical Area 2 Name", "Number of Victimizations")
)
```

### Overall Crime Rate / 1000 Households in Large Areas

```{r visualization-3, cache = TRUE}
total_number_of_households <- sum(df_merge$`Households - Household Count (Total)`)
# Consider a 0.1% of the total household count as the threshold to identify large SA2s.
threshold <- total_number_of_households * 0.001

knitr::kable(
    df_merge |>
        # Filter all NAs
        filter(
            !is.na(Victimisations),
            !is.na(`Households - Household Count (Total)`)
        ) |>
        group_by(sa2_code, sa2_name) |>
        summarize(
            total_victimisations = sum(Victimisations),
            households = sum(`Households - Household Count (Total)`),
            .groups = "drop"
        ) |>
        filter(households > threshold) |>
        mutate(
            # Crime rate per 1,000 households
            crime_rate_per_1000hh = (total_victimisations / households) * 1000
        ) |>
        select(-sa2_code) |>
        arrange(desc(crime_rate_per_1000hh)) |>
        head(n = 10),
    col.names = c("SA2 Name", "Victimizations", "Households", "CR / 1000 HH")
)
```

### Median Household Income vs Total Crimes by Neighborhood

Observation: Number of victimizations is observed to trend downwards with an increase in median household income.

```{r income-vs-crimes, fig.width=10, fig.height=6}
df_merge |>
    group_by(sa2_name) |>
    summarize(
        `Median household income ($)` = median(`Households - Median: Income (Median ($))`, na.rm = TRUE),
        total_crime = sum(Victimisations, na.rm = TRUE)
    ) |>
    ggplot(aes(x = `Median household income ($)`, y = total_crime)) +
    geom_point(alpha = 0.5) +
    geom_smooth(method = "lm") +
    labs(
        title = "Median Household Income vs Total Crimes by Neighborhood",
        x = "Median household income ($)",
        y = "Total Victimisations"
    )
```

### Communication Access vs Total Crimes by Neighborhood

```{r telecom-vs-crimes, fig.width=10, fig.height=6}
df_merge |>
    group_by(sa2_name) |>
    summarize(
        `No Access to Telecom (%)` = sum(`Households - Telecom Access (No access to telecommunication systems)`,
            na.rm = TRUE
        ) /
            sum(`Households - Telecom Access (Total stated)`, na.rm = TRUE) * 100,
        total_crime = sum(Victimisations, na.rm = TRUE)
    ) |>
    ggplot(aes(x = `No Access to Telecom (%)`, y = total_crime)) +
    geom_point(alpha = 0.5) +
    geom_smooth(method = "lm", color = "blue") +
    labs(
        title = "Telecom Access Rate vs Total Crimes by Neighborhood",
        x = "No Access to Telecom (%)",
        y = "Total Victimisations"
    ) +
    theme_minimal()
```

### Crowding Rates vs Total Crimes by Neighborhood

```{r crowding-vs-crimes, fig.width=10, fig.height=6}
df_merge |>
    group_by(sa2_name) |>
    summarize(
        `Crowded (%)` =
            sum(`Households - Crowding Index (Crowded)`, na.rm = TRUE)
            / sum(`Households - Crowding Index (Total stated)`, na.rm = TRUE) * 100,
        `Not Crowded (%)` =
            sum(`Households - Crowding Index (Not crowded)`, na.rm = TRUE)
            / sum(`Households - Crowding Index (Total stated)`, na.rm = TRUE) * 100,
        total_crime = sum(Victimisations, na.rm = TRUE)
    ) |>
    pivot_longer(
        cols = c(`Crowded (%)`, `Not Crowded (%)`),
        names_to = "attribute",
        values_to = "value"
    ) |>
    ggplot(aes(x = value, y = total_crime)) +
    geom_point(alpha = 0.7) +
    geom_smooth(method = "lm", se = FALSE, color = "blue") +
    facet_wrap(~attribute, scales = "free_x") +
    labs(
        title = "Crowding vs Total Crimes by Neighborhood",
        x = NULL,
        y = "Total Victimisations"
    )
```

